{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This notebook allows to reproduce the method results on the KIRP dataset.  \n",
    "We have analyzed the dataset with both GMM and HDBSCAN algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T16:49:06.844579Z",
     "start_time": "2020-08-18T16:48:59.796337Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/scanpy/api/__init__.py:7: FutureWarning: \n",
      "\n",
      "In a future version of Scanpy, `scanpy.api` will be removed.\n",
      "Simply use `import scanpy as sc` and `import scanpy.external as sce` instead.\n",
      "\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "#GPU configuration\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)  # set this TensorFlow session as the default\n",
    "\n",
    "import random\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scripts.data_generator as data_generator\n",
    "import scripts.feature_ranking as feature_ranking\n",
    "import scripts.features_2d as features_2d\n",
    "import scripts.ga as ga\n",
    "import scripts.preprocess as preprocess\n",
    "import scripts.ga_evaluation as ga_evaluation\n",
    "import scripts.bio_analysis as bio_analysis\n",
    "import tensorflow as tf\n",
    "from IPython import get_ipython\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "plt.ion()\n",
    "plt.show()\n",
    "\n",
    "random_state=2\n",
    "random.seed( random_state )\n",
    "np.random.seed(random_state)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T16:49:07.919447Z",
     "start_time": "2020-08-18T16:49:07.846582Z"
    }
   },
   "outputs": [],
   "source": [
    "# truth_column = \"tumor_type\"\n",
    "# truth_values = ['type 1', 'type 2']\n",
    "# filename = \"KIRP\"\n",
    "\n",
    "# df = pd.read_csv(\"../data/rna_data/KIRP.txt\", sep = \"\\t\", low_memory=False)\n",
    "# meta = pd.read_csv(\"../data/rna_data/KIRP_All_CDEs.txt\", sep = \"\\t\", low_memory=False)\n",
    "\n",
    "# preprocess.preprocess_rna(df,\n",
    "#                    meta,\n",
    "#                    truth_column,\n",
    "#                    truth_values,\n",
    "#                    filename,\n",
    "#                    metric='correlation',#'euclidean',\n",
    "#                    normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load preprocessed data\n",
    "\n",
    "## Start here if preprocessing files have been generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T16:49:08.083444Z",
     "start_time": "2020-08-18T16:49:07.928115Z"
    }
   },
   "outputs": [],
   "source": [
    "# filename = \"KIRP\"\n",
    "\n",
    "# data = pd.read_pickle(f\"../data/rna_data/{filename}.pkl\")\n",
    "# # z_file =f\"../data/rna_data/{filename}_Z_correlation.npy\"\n",
    "# # additional_df = pd.read_pickle(f\"../data/rna_data/{filename}_additional.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../../deep_clustering/moduledetection-evaluation/data/ecoli_colombos/E.tsv\", sep = \"\\t\", index_col= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T16:49:08.083444Z",
     "start_time": "2020-08-18T16:49:07.928115Z"
    }
   },
   "outputs": [],
   "source": [
    "# truth = data[\"y\"].values\n",
    "# data = data.drop(\"y\", axis = 1).values\n",
    "# n_clusters = len(np.unique(truth))\n",
    "# Counter(truth), data.shape\n",
    "truth = None\n",
    "n_clusters = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subspace clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T16:24:18.189514Z",
     "start_time": "2020-08-18T16:23:12.098838Z"
    }
   },
   "outputs": [],
   "source": [
    "meta_features = feature_ranking.rank_features(data,\n",
    "                                              nb_bins=20,\n",
    "                                              rank_threshold=90,\n",
    "                                              z_file=None,\n",
    "                                              metric='correlation',\n",
    "                                              redundant_threshold=0.6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T16:27:14.964337Z",
     "start_time": "2020-08-18T16:24:18.193802Z"
    }
   },
   "outputs": [],
   "source": [
    "# model_file = \"../models/gmm_arl.h5\"\n",
    "# gmm_arl_population, n = features_2d.run(data,\n",
    "#                                 n_clusters,\n",
    "#                                 meta_features,\n",
    "#                                 model_file=model_file,\n",
    "#                                 theta=0.1,\n",
    "#                                 add_close_population=False,\n",
    "#                                 exploration_factor = 5)\n",
    "# print(gmm_arl_population.shape, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T16:27:15.034721Z",
     "start_time": "2020-08-18T16:27:14.967712Z"
    }
   },
   "outputs": [],
   "source": [
    "globalResults = {} # Save results for both runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "method = \"adapted_ratkowsky_lance\"\n",
    "score_tolerance=0.009\n",
    "clustering = \"gmm\"\n",
    "\n",
    "round_size = 3\n",
    "debug = False\n",
    "ignore_redundant= True\n",
    "epochs = 10*round_size\n",
    "\n",
    "sampling = {\n",
    "    \"ARCHIVE2D\": { \n",
    "        \"ga\": 0,\n",
    "        \"max\": 0 },\n",
    "    \"CLOSE\": { \n",
    "        \"ga\": 0.35,\n",
    "        \"max\": 0.35 },\n",
    "    \"IMP1D\": { \n",
    "        \"ga\": 0.35,\n",
    "        \"max\": 0.35 },\n",
    "    \"RANDOM\": { \n",
    "        \"ga\": 0.3,\n",
    "        \"max\": 0.3},\n",
    "}\n",
    "params = ga.ga_parameters(\n",
    "    n_clusters,\n",
    "    data.shape[1],\n",
    "    truth,\n",
    "    meta_features,\n",
    "    method=method,\n",
    "    truth_methods=['ari'],\n",
    "    archive_2d=None,\n",
    "    debug=debug,\n",
    "    epochs=epochs,\n",
    "    round_size=round_size,\n",
    "    sampling = sampling,\n",
    "    ignore_redundant = ignore_redundant,\n",
    "    allow_subspace_overlap = False,\n",
    "    improvement_per_mutation_report = True,\n",
    "    score_tolerance=score_tolerance,\n",
    "    clustering = clustering,\n",
    "    total_maximisation_exploration = 400\n",
    "\n",
    ")\n",
    "print(params[\"sampling_actions\"], params[\"maximisation_sizes\"] , params[\"sampling_prob\"])\n",
    "params\n",
    "\n",
    "solutions, archive= ga.run(data, params)\n",
    "solutions.to_pickle(f\"../data/{filename}_{clustering}_{method}.pkl\")\n",
    "display(solutions)\n",
    "# globalResults[f\"{clustering}_{method}\"] = solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"adapted_silhouette\"\n",
    "threshold=0.09\n",
    "score_tolerance=0.009\n",
    "clustering = \"gmm\"\n",
    "\n",
    "round_size = 1#3\n",
    "debug = False\n",
    "ignore_redundant= True\n",
    "epochs = 1#10*round_size\n",
    "\n",
    "sampling = {\n",
    "    \"ARCHIVE2D\": { \n",
    "        \"ga\": 0,\n",
    "        \"max\": 0 },\n",
    "    \"CLOSE\": { \n",
    "        \"ga\": 0.35,\n",
    "        \"max\": 0.35 },\n",
    "    \"IMP1D\": { \n",
    "        \"ga\": 0.35,\n",
    "        \"max\": 0.35 },\n",
    "    \"RANDOM\": { \n",
    "        \"ga\": 0.3,\n",
    "        \"max\": 0.3},\n",
    "}\n",
    "params = ga.ga_parameters(\n",
    "    n_clusters,\n",
    "    data.shape[1],\n",
    "    truth,\n",
    "    meta_features,\n",
    "    method=method,\n",
    "    truth_methods=['ari'],\n",
    "    archive_2d=None,\n",
    "    debug=debug,\n",
    "    epochs=epochs,\n",
    "    round_size=round_size,\n",
    "    sampling = sampling,\n",
    "    ignore_redundant = ignore_redundant,\n",
    "    allow_subspace_overlap = False,\n",
    "    improvement_per_mutation_report = True,\n",
    "    score_tolerance=score_tolerance,\n",
    "    clustering = clustering,\n",
    "    total_maximisation_exploration = 400\n",
    "\n",
    ")\n",
    "print(params[\"sampling_actions\"], params[\"maximisation_sizes\"] , params[\"sampling_prob\"])\n",
    "params\n",
    "\n",
    "solutions, archive= ga.run(data, params)\n",
    "solutions.to_pickle(f\"../data/{filename}_{clustering}_{method}.pkl\")\n",
    "display(solutions)\n",
    "# globalResults[f\"{clustering}_{method}\"] = solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"adapted_silhouette\"\n",
    "threshold=0.09\n",
    "score_tolerance=0.009\n",
    "clustering = \"hdbscan\"\n",
    "\n",
    "round_size = 3\n",
    "debug = False\n",
    "ignore_redundant= True\n",
    "epochs = 10*round_size\n",
    "\n",
    "sampling = {\n",
    "    \"ARCHIVE2D\": { \n",
    "        \"ga\": 0,\n",
    "        \"max\": 0 },\n",
    "    \"CLOSE\": { \n",
    "        \"ga\": 0.35,\n",
    "        \"max\": 0.35 },\n",
    "    \"IMP1D\": { \n",
    "        \"ga\": 0.35,\n",
    "        \"max\": 0.35 },\n",
    "    \"RANDOM\": { \n",
    "        \"ga\": 0.3,\n",
    "        \"max\": 0.3},\n",
    "}\n",
    "params = ga.ga_parameters(\n",
    "    n_clusters,\n",
    "    data.shape[1],\n",
    "    truth,\n",
    "    meta_features,\n",
    "    method=method,\n",
    "    truth_methods=['ari'],\n",
    "    archive_2d=None,\n",
    "    debug=debug,\n",
    "    epochs=epochs,\n",
    "    round_size=round_size,\n",
    "    sampling = sampling,\n",
    "    ignore_redundant = ignore_redundant,\n",
    "    allow_subspace_overlap = False,\n",
    "    improvement_per_mutation_report = True,\n",
    "    score_tolerance=score_tolerance,\n",
    "    clustering = clustering,\n",
    "    total_maximisation_exploration = 500\n",
    "\n",
    ")\n",
    "print(params[\"sampling_actions\"], params[\"maximisation_sizes\"] , params[\"sampling_prob\"])\n",
    "params\n",
    "\n",
    "solutions, archive= ga.run(data, params)\n",
    "solutions.to_pickle(f\"../data/{filename}_{clustering}_{method}.pkl\")\n",
    "display(solutions)\n",
    "globalResults[f\"{clustering}_{method}\"] = solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../../deep_clustering/moduledetection-evaluation/data/ecoli_colombos/knownmodules/ecoli_regulondb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "#     print(f\"Len {len(an[i])}\")\n",
    "    pca = PCA(2)\n",
    "    f = solutions[\"features\"].values[i]\n",
    "    pca_data = pca.fit_transform(data[:, f])\n",
    "    input_data = pca_data\n",
    "\n",
    "#     pred = hdbscan.HDBSCAN(min_cluster_size =10).fit(input_data).labels_\n",
    "    pred = solutions[\"partition\"].values[i]\n",
    "    sil = solutions[\"silhouette\"].values[i]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(f\"Clusters : {np.unique(pred).shape[0]}, Len {len(f)}, silhouette {sil}\")\n",
    "    plt.scatter(input_data[:, 0], input_data[:, 1], c = pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\n",
    "        '../../deep_clustering/moduledetection-evaluation/data/ecoli_colombos/knownmodules/ecoli_regulondb/minimal.json'\n",
    ") as f:\n",
    "    an = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "an = [[int(s[1:]) for s in g] for g in an]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import mixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "#     print(f\"Len {len(an[i])}\")\n",
    "    pca = PCA(2)\n",
    "    input_data = data[:, an[i]]\n",
    "    pca_data = pca.fit_transform(input_data)\n",
    "    \n",
    "\n",
    "    pred = hdbscan.HDBSCAN(min_cluster_size =10).fit(input_data).labels_\n",
    "#     pred = hdbscan.HDBSCAN().fit(input_data).labels_\n",
    "    sil = silhouette_score(input_data, pred)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(f\"Clusters : {np.unique(pred).shape[0]}, Len {len(an[i])}, sil = {sil}\")\n",
    "    plt.scatter(pca_data[:, 0], pca_data[:, 1], c = pred)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"adapted_ratkowsky_lance\"\n",
    "threshold=0.09\n",
    "score_tolerance=0.009\n",
    "clustering = \"hdbscan\"\n",
    "\n",
    "round_size = 3\n",
    "debug = False\n",
    "ignore_redundant= True\n",
    "epochs = 10*round_size\n",
    "\n",
    "sampling = {\n",
    "    \"ARCHIVE2D\": { \n",
    "        \"ga\": 0,\n",
    "        \"max\": 0 },\n",
    "    \"CLOSE\": { \n",
    "        \"ga\": 0.35,\n",
    "        \"max\": 0.35 },\n",
    "    \"IMP1D\": { \n",
    "        \"ga\": 0.35,\n",
    "        \"max\": 0.35 },\n",
    "    \"RANDOM\": { \n",
    "        \"ga\": 0.3,\n",
    "        \"max\": 0.3},\n",
    "}\n",
    "params = ga.ga_parameters(\n",
    "    n_clusters,\n",
    "    data.shape[1],\n",
    "    truth,\n",
    "    meta_features,\n",
    "    method=method,\n",
    "    truth_methods=['ari'],\n",
    "    archive_2d=None,\n",
    "    debug=debug,\n",
    "    epochs=epochs,\n",
    "    round_size=round_size,\n",
    "    sampling = sampling,\n",
    "    ignore_redundant = ignore_redundant,\n",
    "    allow_subspace_overlap = False,\n",
    "    improvement_per_mutation_report = True,\n",
    "    score_tolerance=score_tolerance,\n",
    "    clustering = clustering,\n",
    "    total_maximisation_exploration = 500\n",
    "\n",
    ")\n",
    "print(params[\"sampling_actions\"], params[\"maximisation_sizes\"] , params[\"sampling_prob\"])\n",
    "params\n",
    "\n",
    "solutions, archive= ga.run(data, params)\n",
    "solutions.to_pickle(f\"../data/{filename}_{clustering}_{method}.pkl\")\n",
    "display(solutions)\n",
    "globalResults[f\"{clustering}_{method}\"] = solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T10:13:38.096393Z",
     "start_time": "2020-07-12T10:13:37.949675Z"
    }
   },
   "source": [
    "# Interpret results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T16:35:55.238342Z",
     "start_time": "2020-08-18T16:35:50.481397Z"
    }
   },
   "outputs": [],
   "source": [
    "additional_results, best_subspace_match, best_meta_subspace = bio_analysis.clinical_data_analysis(\n",
    "    additional_df, solutions, n_clusters)\n",
    "\n",
    "best_subspace_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T16:41:25.439076Z",
     "start_time": "2020-08-18T16:35:55.240324Z"
    }
   },
   "outputs": [],
   "source": [
    "method = \"adapted_silhouette\"\n",
    "threshold=0.1\n",
    "score_tolerance=0.01\n",
    "clustering = \"hdbscan\"\n",
    "\n",
    "round_size = 3\n",
    "debug = False\n",
    "ignore_redundant= True\n",
    "epochs = 10*round_size\n",
    "\n",
    "sampling = {\n",
    "    \"ARCHIVE2D\": { \n",
    "        \"ga\": 0.3,\n",
    "        \"max\": 0.3 },\n",
    "    \"CLOSE\": { \n",
    "        \"ga\": 0.4,\n",
    "        \"max\": 0.4 },\n",
    "    \"IMP1D\": { \n",
    "        \"ga\": 0.2,\n",
    "        \"max\": 0.2 },\n",
    "    \"RANDOM\": { \n",
    "        \"ga\": 0.1,\n",
    "        \"max\": 0.1},\n",
    "}\n",
    "params = ga.ga_parameters(\n",
    "    n_clusters,\n",
    "    data.shape[1],\n",
    "    truth,\n",
    "    meta_features,\n",
    "    method=method,\n",
    "    truth_methods=['ari'],\n",
    "    archive_2d=gmm_arl_population[gmm_arl_population[\"pred\"] > threshold].iloc[:7000],\n",
    "    debug=debug,\n",
    "    epochs=epochs,\n",
    "    round_size=round_size,\n",
    "    sampling = sampling,\n",
    "    ignore_redundant = ignore_redundant,\n",
    "    allow_subspace_overlap = False,\n",
    "    improvement_per_mutation_report = True,\n",
    "    score_tolerance=score_tolerance,\n",
    "    clustering = clustering,\n",
    "    total_maximisation_exploration = 500\n",
    "\n",
    ")\n",
    "print(params[\"sampling_actions\"], params[\"maximisation_sizes\"] , params[\"sampling_prob\"])\n",
    "params\n",
    "\n",
    "solutions, archive= ga.run(data, params)\n",
    "solutions.to_pickle(f\"../data/{filename}_{clustering}_{method}.pkl\")\n",
    "display(solutions)\n",
    "globalResults[f\"{clustering}_{method}\"] = solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpret results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T16:41:30.721041Z",
     "start_time": "2020-08-18T16:41:25.440622Z"
    }
   },
   "outputs": [],
   "source": [
    "additional_results, best_subspace_match, best_meta_subspace = bio_analysis.clinical_data_analysis(\n",
    "    additional_df, solutions, n_clusters)\n",
    "\n",
    "best_subspace_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T16:52:43.514637Z",
     "start_time": "2020-08-18T16:52:43.474880Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import mixture\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T16:41:50.029440Z",
     "start_time": "2020-08-18T16:41:30.791075Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ranked_features = feature_ranking.supervised_feature_ranking(data, truth, \n",
    "                        nbTopFeatures = data.shape[1])\n",
    "data = data[:, ranked_features]\n",
    "imp_f = np.arange(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T16:41:51.798022Z",
     "start_time": "2020-08-18T16:41:50.031948Z"
    }
   },
   "outputs": [],
   "source": [
    "gmm_scores = []\n",
    "hdbscan_scores = []\n",
    "for i in range(2, 50):\n",
    "    input_data = data[:, :i]\n",
    "    gmm = mixture.GaussianMixture(n_components=n_clusters,\n",
    "                      covariance_type=\"full\", random_state=0)\n",
    "    pred = gmm.fit_predict(input_data)\n",
    "    ari = adjusted_rand_score(truth, pred)\n",
    "    gmm_scores.append(ari)\n",
    "\n",
    "    pred = hdbscan.HDBSCAN(min_cluster_size =2).fit(input_data).labels_\n",
    "    ari = adjusted_rand_score(truth, pred)\n",
    "    hdbscan_scores.append(ari)\n",
    "print(f\" GMM ari = {max(gmm_scores)}, {np.argmax(gmm_scores)}\")\n",
    "print(f\" HDBSCAN ari = {max(hdbscan_scores)}, {np.argmax(hdbscan_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T16:53:46.120822Z",
     "start_time": "2020-08-18T16:52:48.264600Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2,  mutual_info_classif, SelectKBest\n",
    "sel = SelectKBest(mutual_info_classif, k=50).fit_transform(data, truth)\n",
    "gmm_scores = []\n",
    "hdbscan_scores = []\n",
    "for i in range(2, 50):\n",
    "    input_data = sel[:, :i]\n",
    "    gmm = mixture.GaussianMixture(n_components=n_clusters,\n",
    "                      covariance_type=\"full\", random_state=0)\n",
    "    pred = gmm.fit_predict(input_data)\n",
    "    ari = adjusted_rand_score(truth, pred)\n",
    "    gmm_scores.append(ari)\n",
    "\n",
    "    pred = hdbscan.HDBSCAN(min_cluster_size =2).fit(input_data).labels_\n",
    "    ari = adjusted_rand_score(truth, pred)\n",
    "    hdbscan_scores.append(ari)\n",
    "print(f\" GMM ari = {max(gmm_scores)}, {np.argmax(gmm_scores)}\")\n",
    "print(f\" HDBSCAN ari = {max(hdbscan_scores)}, {np.argmax(hdbscan_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-18T16:23:07.708Z"
    }
   },
   "outputs": [],
   "source": [
    "input_data = data\n",
    "gmm = mixture.GaussianMixture(n_components=n_clusters,\n",
    "                      covariance_type=\"full\", random_state=0)\n",
    "pred = gmm.fit_predict(input_data)\n",
    "ari = adjusted_rand_score(truth, pred)\n",
    "print(f\"GMM ari = {ari}\")\n",
    "\n",
    "\n",
    "pred = KMeans(n_clusters= n_clusters).fit(input_data).labels_\n",
    "ari = adjusted_rand_score(truth, pred)\n",
    "print(f\"Kmeans ari = {ari}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-18T16:23:07.711Z"
    }
   },
   "outputs": [],
   "source": [
    "input_data = data\n",
    "pred = hdbscan.HDBSCAN(min_cluster_size =2).fit(input_data).labels_\n",
    "ari = adjusted_rand_score(truth, pred)\n",
    "print(f\"HDBSCAN ari {ari}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-18T16:23:07.714Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predict on PCA\n",
    "pca = PCA(2)\n",
    "pca_data = pca.fit_transform(data)\n",
    "input_data = pca_data\n",
    "gmm = mixture.GaussianMixture(n_components=n_clusters,\n",
    "                      covariance_type=\"full\", random_state=0)\n",
    "pred = gmm.fit_predict(input_data)\n",
    "ari = adjusted_rand_score(truth, pred)\n",
    "print(f\"GMM ari = {ari}\")\n",
    "\n",
    "pred = hdbscan.HDBSCAN(min_cluster_size =2).fit(input_data).labels_\n",
    "ari = adjusted_rand_score(truth, pred)\n",
    "print(f\"HDBSCAN ari = {ari}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import mixture\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import KMeans\n",
    "import hdbscan\n",
    "\n",
    "row = {}\n",
    "clustering = AffinityPropagation(random_state=5).fit(data)\n",
    "ari = adjusted_rand_score(truth, clustering.labels_)\n",
    "print(f\"Affinity {ari}\")\n",
    "row[\"AffinityPropagation\"] = ari\n",
    "\n",
    "clustering = SpectralClustering(n_clusters=n_clusters, random_state=0).fit(data)\n",
    "ari = adjusted_rand_score(truth, clustering.labels_)\n",
    "print(f\"Spectral {ari}\")\n",
    "row[\"Spectral\"] = ari\n",
    "\n",
    "clustering = KMeans(n_clusters=n_clusters,random_state=5).fit(data)\n",
    "ari = adjusted_rand_score(truth, clustering.labels_)\n",
    "print(f\"KMeans {ari}\")\n",
    "row[\"KMeans\"] = ari\n",
    "\n",
    "gmm = mixture.GaussianMixture(n_components=n_clusters,\n",
    "              covariance_type=\"full\", random_state=0)\n",
    "pred = gmm.fit_predict(data[:, :8000])\n",
    "ari = adjusted_rand_score(truth, pred)\n",
    "print(f\"GMM {ari}\")\n",
    "row[\"GMM\"] = ari\n",
    "\n",
    "pred = hdbscan.HDBSCAN(min_cluster_size =2).fit(data).labels_\n",
    "ari = adjusted_rand_score(truth, pred)\n",
    "print(f\"HDBSCAN {ari}\")\n",
    "row[\"HDBSCAN\"] = ari\n",
    "\n",
    "pca = PCA(2)\n",
    "pca_data = pca.fit_transform(data)\n",
    "\n",
    "clustering = KMeans(n_clusters=n_clusters,random_state=5).fit(pca_data)\n",
    "ari = adjusted_rand_score(truth, clustering.labels_)\n",
    "print(f\"PCA KMeans {ari}\")\n",
    "row[\"PCA_KMeans\"] = ari\n",
    "\n",
    "gmm = mixture.GaussianMixture(n_components=n_clusters,\n",
    "              covariance_type=\"full\", random_state=0)\n",
    "pred = gmm.fit_predict(pca_data)\n",
    "ari = adjusted_rand_score(truth, pred)\n",
    "print(f\"PCA GMM {ari}\")\n",
    "row[\"PCA_GMM\"] = ari\n",
    "\n",
    "pred = hdbscan.HDBSCAN(min_cluster_size =2).fit(pca_data).labels_\n",
    "ari = adjusted_rand_score(truth, pred)\n",
    "print(f\"PCAHDBSCAN {ari}\")\n",
    "row[\"PCA_HDBSCAN\"] = ari\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 361.5,
   "position": {
    "height": "40px",
    "left": "1035px",
    "right": "20px",
    "top": "120px",
    "width": "313px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
