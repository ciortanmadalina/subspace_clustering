{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary \n",
    "This notebook allows to replicate the generation of data used for training the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T10:23:50.545730Z",
     "start_time": "2020-07-10T10:23:48.053991Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "#GPU configuration\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)  # set this TensorFlow session as the default\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.cluster import  KMeans, MeanShift\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "import scipy\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import sys\n",
    "from keras.utils import to_categorical\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "from datetime import datetime\n",
    "import time\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import scripts.data_generator as data_generator\n",
    "import scripts.internal_scores as validation\n",
    "import scripts.cnn_models as cnn_models\n",
    "import hdbscan\n",
    "from sklearn import mixture\n",
    "import scripts.plot_losses as plot_losses\n",
    "random_state=0\n",
    "random.seed( random_state )\n",
    "np.random.seed(random_state)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data simulation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T10:23:52.033832Z",
     "start_time": "2020-07-10T10:23:51.912079Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_subspace(subspace,\n",
    "                      all_x,\n",
    "                      all_y,\n",
    "                      all_original_input,\n",
    "                      return_data=True,\n",
    "                      methods=[\"adapted_ratkowsky_lance\", \"adapted_silhouette\"],\n",
    "                      nb_bins=20,\n",
    "                      n_clusters=2):\n",
    "    \"\"\"\n",
    "    Compute data score and add results to global all_x, all_y, all_original_input\n",
    "    \"\"\"\n",
    "    subspace = preprocessing.MinMaxScaler().fit_transform(subspace)\n",
    "    if return_data:\n",
    "        all_original_input.append(subspace)\n",
    "    scores = []\n",
    "    predK = KMeans(n_clusters=n_clusters, random_state=0).fit(subspace).labels_\n",
    "    for method in methods:\n",
    "        score = round(getattr(validation.validation(), method)(subspace, predK), 4)\n",
    "        scores.append(score)\n",
    "        \n",
    "    predK = hdbscan.HDBSCAN(min_cluster_size =10).fit(subspace).labels_\n",
    "    for method in methods:\n",
    "        if len(np.unique(predK)) == 1: #only one cluster\n",
    "            score = 0\n",
    "        else:\n",
    "            score = round(getattr(validation.validation(), method)(subspace, predK), 4)\n",
    "        scores.append(score)\n",
    "        \n",
    "    gmm = mixture.GaussianMixture(n_components=n_clusters,\n",
    "                      covariance_type=\"full\", random_state=0)\n",
    "    predK = gmm.fit_predict(subspace)\n",
    "    for method in methods:\n",
    "        if len(np.unique(predK)) == 1: #only one cluster\n",
    "            score = 0\n",
    "        else:\n",
    "            score = round(getattr(validation.validation(), method)(subspace, predK), 4)\n",
    "        scores.append(score)\n",
    "        \n",
    "    digitized_subspace = cnn_models.digitize(subspace, nb_bins=20)\n",
    "    img = cnn_models.digitized_subspace_to_img(digitized_subspace, nb_bins)\n",
    "\n",
    "    all_x.append(img)\n",
    "    all_y.loc[all_y.shape[0]] = scores\n",
    "\n",
    "    return all_x, all_y, all_original_input\n",
    "\n",
    "def generate_blobs(size,\n",
    "                   all_n_clusters=[3],\n",
    "                   nb_bins=20,\n",
    "                   return_data=True,\n",
    "                   methods=[\"adapted_ratkowsky_lance\", \"adapted_silhouette\", \"Wemmert_Gancarski\"],\n",
    "                   n_clusters=2):\n",
    "    all_img = []\n",
    "    all_y = pd.DataFrame(columns = [\"km_arl\", \"km_as\",\n",
    "                                    \"km_wg\", \"h_arl\", \"h_as\",\"h_wg\", \"gmm_arl\", \"gmm_as\", \"gmm_wg\"])\n",
    "\n",
    "    all_original_input = []\n",
    "\n",
    "    # 1. Generate blob subpaces\n",
    "    for nc in all_n_clusters:\n",
    "        print(f\"Generating {size} gaussian blobs with {nc} clusters\")\n",
    "\n",
    "        for i in range(size):\n",
    "            n_samples = max(np.random.randint(15, 200) * nc, 150)\n",
    "            n_samples = min(n_samples, 1500)\n",
    "            data, _, _ = data_generator.make_blob_data(n_samples,\n",
    "                                                  [nc],\n",
    "                                                  None,\n",
    "                                                  2, 7, plot = False)\n",
    "#             n_cutoff= np.random.choice([1,2])\n",
    "#             data = data_generator.cutoff_data(data, n_cutoff)\n",
    "            ids = list(itertools.combinations(np.arange(data.shape[1]), 2))\n",
    "\n",
    "            for i, j in ids:\n",
    "                \n",
    "                all_img, all_y, all_original_input = evaluate_subspace(\n",
    "                    data[:, [i, j]],\n",
    "                    all_img,\n",
    "                    all_y,\n",
    "                    all_original_input,\n",
    "                    return_data=return_data,\n",
    "                    nb_bins=nb_bins,\n",
    "                    methods=methods,\n",
    "                    n_clusters=n_clusters)\n",
    "\n",
    "\n",
    "    all_y[\"id\"] = np.arange(all_y.shape[0])\n",
    "    return np.array(all_img), all_y, np.array(all_original_input)\n",
    "\n",
    "\n",
    "def generate_mixed_data(n_experiments=20,\n",
    "                        nb_bins=20,\n",
    "                        return_data=True,\n",
    "                        n_clusters_subspace=[2, 3],\n",
    "                        n_obs_mixed_distribution=3,\n",
    "                        methods=[\"adapted_ratkowsky_lance\", \"adapted_silhouette\", \"Wemmert_Gancarski\"],\n",
    "                        data_file=\"../data/mixed_train_img_2d.npy\",\n",
    "                        score_file=\"../data/mixed_train_2d.pkl\",\n",
    "                        orig_file = \"\",\n",
    "                        n_clusters=2):\n",
    "    all_img = []\n",
    "    all_y = pd.DataFrame(columns = [\"km_arl\", \"km_as\",\n",
    "                                    \"km_wg\", \"h_arl\", \"h_as\",\"h_wg\", \"gmm_arl\", \"gmm_as\", \"gmm_wg\"])\n",
    "    all_original_input = []\n",
    "\n",
    "    for _ in range(n_experiments):\n",
    "        subspace_clusters = np.random.choice(np.arange(2, 9), 2, replace=False)\n",
    "        n_samples = max(150, np.random.randint(10, 150) * max(subspace_clusters))\n",
    "        n_samples = min(n_samples, 1500)\n",
    "        mixed_data, _, _ = data_generator.make_data_for_ga(\n",
    "                     subspace_clusters,\n",
    "                     cluster_std=None,\n",
    "                     n_uniform_features=3,\n",
    "                     n_normal_features=3,\n",
    "                     n_neg_binomial=4,\n",
    "                     n_gamma=3,\n",
    "                     n_beta=7,\n",
    "                     random_redundant=True,\n",
    "                     n_redundant=5,\n",
    "                     n_outlier_features=2,\n",
    "                     n_cutoff=4,\n",
    "                     n_bimodal_features=2,\n",
    "                     min_subspace_features=2,\n",
    "                     max_subspace_features=6,\n",
    "                     n_samples= n_samples,\n",
    "                     plot=False)\n",
    "        print(\n",
    "            f\"Mixing subpaces of {mixed_data.shape}, subspaces with {subspace_clusters} clusters\"\n",
    "        )\n",
    "        for i, j in tqdm(\n",
    "                itertools.combinations(np.arange(mixed_data.shape[1]), 2)):\n",
    "            all_img, all_y, all_original_input = evaluate_subspace(\n",
    "                mixed_data[:, [i, j]],\n",
    "                all_img,\n",
    "                all_y,\n",
    "                all_original_input,\n",
    "                return_data=return_data,\n",
    "                nb_bins=nb_bins,\n",
    "                methods=methods,\n",
    "                n_clusters=n_clusters)\n",
    "\n",
    "\n",
    "    all_y[\"id\"] = np.arange(all_y.shape[0])\n",
    "    np.save(data_file, np.array(all_img))\n",
    "    np.save(orig_file, np.array(all_original_input))\n",
    "    all_y.to_pickle(score_file)\n",
    "    return np.array(all_img), all_y, np.array(all_original_input)\n",
    "\n",
    "\n",
    "def generate_biological_subspaces(filenames,\n",
    "                                  methods=[\"adapted_ratkowsky_lance\", \"adapted_silhouette\", \"Wemmert_Gancarski\"],\n",
    "                                  n_subspaces=1000,\n",
    "                                  nb_bins=20,\n",
    "                                  return_data = True,\n",
    "                                  truth_column=\"truth\",\n",
    "                                  n_clusters=2):\n",
    "    \"\"\"\n",
    "    Samples from the list of biological subspaces n_subspaces 2 d random\n",
    "    combinations\n",
    "    \"\"\"\n",
    "    all_img = []\n",
    "    all_y = pd.DataFrame(columns = [\"km_arl\", \"km_as\",\n",
    "                                    \"km_wg\", \"h_arl\", \"h_as\",\"h_wg\", \"gmm_arl\", \"gmm_as\", \"gmm_wg\"])\n",
    "    all_original_input = []\n",
    "    for filename in filenames:\n",
    "        print(filename)\n",
    "        data = pd.read_pickle(filename)\n",
    "        truth = data[truth_column].values\n",
    "        data = data.drop(truth_column, axis=1).values\n",
    "        if n_clusters>=data.shape[0]/2:\n",
    "            continue\n",
    "        subspaces = np.random.choice(np.arange(data.shape[1]),\n",
    "                                     size=(n_subspaces, 2))\n",
    "        \n",
    "        for subspace in subspaces:\n",
    "            all_img, all_y, all_original_input = evaluate_subspace(\n",
    "                data[:, subspace],\n",
    "                all_img,\n",
    "                all_y,\n",
    "                all_original_input,\n",
    "                return_data=return_data,\n",
    "                nb_bins=nb_bins,\n",
    "                methods=methods,\n",
    "                n_clusters=n_clusters)\n",
    "\n",
    "    all_y[\"id\"] = np.arange(all_y.shape[0])\n",
    "    if len(all_img) == 0:\n",
    "        return np.zeros((0, 21, 21, 1)), all_y, np.array(all_original_input)\n",
    "    return np.array(all_img), all_y, np.array(all_original_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T10:23:53.328230Z",
     "start_time": "2020-07-10T10:23:53.223316Z"
    }
   },
   "outputs": [],
   "source": [
    "nb_bins = 20\n",
    "all_n_clusters = np.arange(2,30)\n",
    "\n",
    "for n_clusters in all_n_clusters:\n",
    "    print(\">>\", n_clusters)\n",
    "    # Blob datasets\n",
    "    blob_train_img, blob_train_scores, blob_train_orig = generate_blobs(\n",
    "        7, all_n_clusters=all_n_clusters, nb_bins=20, n_clusters=n_clusters, return_data=True)\n",
    "    np.save(f\"../data/nn_data/blob_train_img_{n_clusters}.npy\", blob_train_img)\n",
    "    np.save(f\"../data/nn_data/blob_train_orig_{n_clusters}.npy\", blob_train_orig)\n",
    "    blob_train_scores.to_pickle(f\"../data/nn_data/blob_train_y_{n_clusters}.pkl\")\n",
    "\n",
    "    blob_val_img, blob_val_scores, blob_val_orig = generate_blobs(\n",
    "        1, all_n_clusters=all_n_clusters, nb_bins=20, n_clusters=n_clusters, return_data=True,)\n",
    "\n",
    "    np.save(f\"../data/nn_data/blob_val_img_{n_clusters}.npy\", blob_val_img)\n",
    "    np.save(f\"../data/nn_data/blob_val_orig_{n_clusters}.npy\", blob_val_orig)\n",
    "    blob_val_scores.to_pickle(f\"../data/nn_data/blob_val_y_{n_clusters}.pkl\")\n",
    "    \n",
    "#     # Mixed features\n",
    "    mixed_train_img, mixed_train_scores, _ = generate_mixed_data(\n",
    "        n_experiments=10,\n",
    "        nb_bins=20,\n",
    "        n_clusters_subspace=all_n_clusters,\n",
    "        n_obs_mixed_distribution=2,\n",
    "        return_data=True,\n",
    "        data_file=f\"../data/nn_data1/mixed_train_img_{n_clusters}.npy\",\n",
    "        score_file=f\"../data/nn_data1/mixed_train_y_{n_clusters}.pkl\",\n",
    "        orig_file = f\"../data/nn_data1/mixed_train_orig_{n_clusters}.npy\",\n",
    "        n_clusters=n_clusters)\n",
    "\n",
    "    mixed_val_img, mixed_val_scores, _ = generate_mixed_data(\n",
    "        n_experiments=1,\n",
    "        nb_bins=20,\n",
    "        n_clusters_subspace=all_n_clusters,\n",
    "        n_obs_mixed_distribution=2,\n",
    "        return_data=True,\n",
    "        data_file=f\"../data/nn_data/mixed_val_img_{n_clusters}.npy\",\n",
    "        score_file=f\"../data/nn_data/mixed_val_y_{n_clusters}.pkl\",\n",
    "        orig_file = f\"../data/nn_data/mixed_val_orig_{n_clusters}.npy\",\n",
    "        n_clusters=n_clusters)\n",
    "\n",
    "\n",
    "    filenames = [\n",
    "        '../data/microarray/alon.pkl', '../data/microarray/nakayama.pkl',\n",
    "        '../data/microarray/christensen.pkl', '../data/microarray/west.pkl'\n",
    "    ]\n",
    "\n",
    "    bio_img_train, bio_scores_train, bio_orig_train = generate_biological_subspaces(\n",
    "        filenames,\n",
    "        n_subspaces=100,\n",
    "        nb_bins=20,\n",
    "        n_clusters=n_clusters)\n",
    "\n",
    "    np.save(f\"../data/nn_data/ma_train_img_{n_clusters}.npy\", bio_img_train)\n",
    "    np.save(f\"../data/nn_data/ma_train_orig_{n_clusters}.npy\", bio_orig_train)\n",
    "    bio_scores_train.to_pickle(f\"../data/nn_data/ma_train_y_{n_clusters}.pkl\")\n",
    "\n",
    "    filenames = ['../data/microarray/alon.pkl', '../data/microarray/nakayama.pkl']\n",
    "    bio_img_val, bio_scores_val, bio_orig_val = generate_biological_subspaces(\n",
    "        filenames,\n",
    "        n_subspaces=10,\n",
    "        nb_bins=20,\n",
    "        n_clusters=n_clusters)\n",
    "\n",
    "    np.save(f\"../data/nn_data/ma_val_img_{n_clusters}.npy\", bio_img_val)\n",
    "    np.save(f\"../data/nn_data/ma_val_orig_{n_clusters}.npy\", bio_orig_val)\n",
    "    bio_scores_val.to_pickle(f\"../data/nn_data/ma_val_y_{n_clusters}.pkl\")\n",
    "    filenames = [\n",
    "        '../data/microarray/su.pkl', '../data/microarray/chin.pkl',\n",
    "        '../data/microarray/tian.pkl'\n",
    "    ]\n",
    "\n",
    "    bio_img_test, bio_scores_test, bio_test_orig = generate_biological_subspaces(\n",
    "        filenames,\n",
    "        n_subspaces=50, nb_bins=20, n_clusters=n_clusters)\n",
    "\n",
    "    np.save(f\"../data/nn_data/ma_test_img_{n_clusters}.npy\", bio_img_test)\n",
    "    np.save(f\"../data/nn_data/ma_test_orig_{n_clusters}.npy\", bio_test_orig)\n",
    "    bio_scores_test.to_pickle(f\"../data/nn_data/ma_test_y_{n_clusters}.pkl\")\n",
    "\n",
    "    # RNA seq data\n",
    "    filenames = [\n",
    "        '../data/rna_data/BRCA.pkl', '../data/rna_data/KIRP.pkl'\n",
    "    ]\n",
    "\n",
    "    bio_img_test, bio_scores_test, bio_test_orig = generate_biological_subspaces(\n",
    "        filenames, n_subspaces=50, nb_bins=20,\n",
    "        truth_column = \"y\", n_clusters=n_clusters)\n",
    "\n",
    "    np.save(f\"../data/nn_data/rna_test_img_{n_clusters}.npy\", bio_img_test)\n",
    "    np.save(f\"../data/nn_data/rna_test_orig_{n_clusters}.npy\", bio_test_orig)\n",
    "    bio_scores_test.to_pickle(f\"../data/nn_data/rna_test_y_{n_clusters}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_n_clusters = np.arange(2,20)\n",
    "num_classes = len(all_n_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_img = []\n",
    "x_train_orig = []\n",
    "x_train_k = []\n",
    "y_train = []\n",
    "\n",
    "for n_clusters in tqdm(all_n_clusters):\n",
    "    mixed_train_img = np.load(\n",
    "        f\"../data/nn_data/mixed_train_img_{n_clusters}.npy\")\n",
    "    mixed_train_orig = np.load(\n",
    "        f\"../data/nn_data/mixed_train_orig_{n_clusters}.npy\", allow_pickle=True)\n",
    "    mixed_train_scores = pd.read_pickle(\n",
    "        f\"../data/nn_data/mixed_train_y_{n_clusters}.pkl\")\n",
    "    \n",
    "    mixed_train_img1 = np.load(\n",
    "        f\"../data/nn_data1/mixed_train_img_{n_clusters}.npy\")\n",
    "    mixed_train_orig1 = np.load(\n",
    "        f\"../data/nn_data1/mixed_train_orig_{n_clusters}.npy\", allow_pickle=True)\n",
    "    mixed_train_scores1 = pd.read_pickle(\n",
    "        f\"../data/nn_data1/mixed_train_y_{n_clusters}.pkl\")\n",
    "    \n",
    "    blob_train_img = np.load(\n",
    "        f\"../data/nn_data/blob_train_img_{n_clusters}.npy\")\n",
    "    blob_train_orig = np.load(\n",
    "        f\"../data/nn_data/blob_train_orig_{n_clusters}.npy\",  allow_pickle=True)\n",
    "    blob_train_scores = pd.read_pickle(\n",
    "        f\"../data/nn_data/blob_train_y_{n_clusters}.pkl\")\n",
    "    \n",
    "    \n",
    "    bio_train_img = np.load(\n",
    "        f\"../data/nn_data/ma_train_img_{n_clusters}.npy\")\n",
    "    bio_orig_train = np.load(\n",
    "        f\"../data/nn_data/ma_train_orig_{n_clusters}.npy\", allow_pickle=True)\n",
    "    bio_train_scores = pd.read_pickle(\n",
    "        f\"../data/nn_data/ma_train_y_{n_clusters}.pkl\")\n",
    "    \n",
    "    x_train_orig.extend(list(mixed_train_orig))\n",
    "    x_train_orig.extend(list(mixed_train_orig1))\n",
    "    x_train_orig.extend(list(blob_train_orig))\n",
    "    x_train_orig.extend(list(bio_orig_train))\n",
    "    \n",
    "    x_train_img.append(\n",
    "            np.concatenate([mixed_train_img,mixed_train_img1, blob_train_img, \n",
    "                            bio_train_img\n",
    "                           ]))\n",
    "    train_scores = pd.concat(\n",
    "            [mixed_train_scores, mixed_train_scores1, blob_train_scores, \n",
    "             bio_train_scores\n",
    "            ])\n",
    "    x_train_k.append([n_clusters-2] * train_scores.shape[0])\n",
    "    y_train.append(train_scores)\n",
    "    \n",
    "\n",
    "\n",
    "x_train_img = np.concatenate(x_train_img)\n",
    "x_train_orig = np.array(x_train_orig)\n",
    "y_train=pd.concat(y_train)\n",
    "y_train[\"id\"] = np.arange(y_train.shape[0])\n",
    "# x_train_k = np.concatenate(x_train_k)\n",
    "x_train_k = to_categorical(np.concatenate(x_train_k), num_classes=num_classes)\n",
    "print (x_train_img.shape, len(x_train_orig), y_train.shape, x_train_k.shape)\n",
    "np.save(\"../data/nn_data/img_x_train_img.npy\", x_train_img)\n",
    "np.save(\"../data/nn_data/img_x_train_k.npy\", x_train_k)\n",
    "y_train.to_pickle(\"../data/nn_data/img_y_train.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x_train_img, x_train_orig, y_train, x_train_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_img = []\n",
    "x_val_orig = []\n",
    "x_val_k = []\n",
    "y_val = []\n",
    "\n",
    "for n_clusters in tqdm(all_n_clusters):\n",
    "    mixed_val_img = np.load(\n",
    "        f\"../data/nn_data/mixed_val_img_{n_clusters}.npy\")\n",
    "    mixed_val_orig = np.load(\n",
    "        f\"../data/nn_data/mixed_val_orig_{n_clusters}.npy\", allow_pickle=True)\n",
    "    mixed_val_scores = pd.read_pickle(\n",
    "        f\"../data/nn_data/mixed_val_y_{n_clusters}.pkl\")\n",
    "    \n",
    "    blob_val_img = np.load(\n",
    "        f\"../data/nn_data/blob_val_img_{n_clusters}.npy\")\n",
    "    blob_val_orig = np.load(\n",
    "        f\"../data/nn_data/blob_val_orig_{n_clusters}.npy\",  allow_pickle=True)\n",
    "    blob_val_scores = pd.read_pickle(\n",
    "        f\"../data/nn_data/blob_val_y_{n_clusters}.pkl\")\n",
    "    \n",
    "    \n",
    "    bio_val_img = np.load(\n",
    "        f\"../data/nn_data/ma_val_img_{n_clusters}.npy\")\n",
    "    bio_orig_val = np.load(\n",
    "        f\"../data/nn_data/ma_val_orig_{n_clusters}.npy\", allow_pickle=True)\n",
    "    bio_val_scores = pd.read_pickle(\n",
    "        f\"../data/nn_data/ma_val_y_{n_clusters}.pkl\")\n",
    "    \n",
    "    x_val_orig.extend(list(mixed_val_orig))\n",
    "    x_val_orig.extend(list(blob_val_orig))\n",
    "    x_val_orig.extend(list(bio_orig_val))\n",
    "    \n",
    "    x_val_img.append(\n",
    "            np.concatenate([mixed_val_img, blob_val_img, \n",
    "                            bio_val_img\n",
    "                           ]))\n",
    "    val_scores = pd.concat(\n",
    "            [mixed_val_scores, blob_val_scores, \n",
    "             bio_val_scores\n",
    "            ])\n",
    "    x_val_k.append([n_clusters-2] * val_scores.shape[0])\n",
    "    y_val.append(val_scores)\n",
    "    \n",
    "\n",
    "\n",
    "x_val_img = np.concatenate(x_val_img)\n",
    "x_val_orig = np.array(x_val_orig)\n",
    "y_val=pd.concat(y_val)\n",
    "y_val[\"id\"] = np.arange(y_val.shape[0])\n",
    "# x_val_k = np.concatenate(x_val_k)\n",
    "x_val_k = to_categorical(np.concatenate(x_val_k), num_classes=num_classes)\n",
    "print (x_val_img.shape, len(x_val_orig), y_val.shape, x_val_k.shape)\n",
    "np.save(\"../data/nn_data/img_x_val_img.npy\", x_val_img)\n",
    "np.save(\"../data/nn_data/img_x_val_k.npy\", x_val_k)\n",
    "y_val.to_pickle(\"../data/nn_data/img_y_val.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_ma_img = []\n",
    "x_test_ma_orig = []\n",
    "x_test_ma_k = []\n",
    "y_test_ma = []\n",
    "\n",
    "x_test_rna_img = []\n",
    "x_test_rna_orig = []\n",
    "x_test_rna_k = []\n",
    "y_test_rna = []\n",
    "\n",
    "# for n_clusters in tqdm(all_n_clusters):\n",
    "for n_clusters in tqdm(np.arange(2, 10)):\n",
    "    bio_img_test_ma = np.load(\n",
    "        f\"../data/nn_data/ma_test_img_{n_clusters}.npy\")\n",
    "    bio_scores_test_ma = pd.read_pickle(\n",
    "        f\"../data/nn_data/ma_test_y_{n_clusters}.pkl\")\n",
    "    \n",
    "    x_test_ma_img.append(bio_img_test_ma)\n",
    "    x_test_ma_k.append([n_clusters-2] * bio_scores_test_ma.shape[0])\n",
    "    y_test_ma.append(bio_scores_test_ma)\n",
    "\n",
    "    bio_img_test = np.load(\n",
    "        f\"../data/nn_data/rna_test_img_{n_clusters}.npy\")\n",
    "    bio_scores_test = pd.read_pickle(\n",
    "        f\"../data/nn_data/rna_test_y_{n_clusters}.pkl\")\n",
    "    \n",
    "    x_test_rna_img.append(bio_img_test)\n",
    "    x_test_rna_k.append([n_clusters-2] * bio_scores_test.shape[0])\n",
    "    y_test_rna.append(bio_scores_test)\n",
    "\n",
    "x_test_ma_img = np.concatenate(x_test_ma_img)\n",
    "y_test_ma=pd.concat(y_test_ma)\n",
    "\n",
    "y_test_ma[\"id\"] = np.arange(y_test_ma.shape[0])\n",
    "x_test_ma_k = to_categorical(np.concatenate(x_test_ma_k), num_classes=num_classes)\n",
    "print(x_test_ma_img.shape, y_test_ma.shape, x_test_ma_k.shape)\n",
    "\n",
    "x_test_rna_img = np.concatenate(x_test_rna_img)\n",
    "y_test_rna=pd.concat(y_test_rna)\n",
    "y_test_rna[\"id\"] = np.arange(y_test_rna.shape[0])\n",
    "x_test_rna_k = to_categorical(np.concatenate(x_test_rna_k), num_classes=num_classes)\n",
    "print (x_test_rna_img.shape, y_test_rna.shape, x_test_rna_k.shape)\n",
    "\n",
    "np.save(\"../data/nn_data/img_x_test_ma_img.npy\", x_test_ma_img)\n",
    "np.save(\"../data/nn_data/img_x_test_ma_k.npy\", x_test_ma_k)\n",
    "y_test_ma.to_pickle(\"../data/nn_data/img_y_test_ma.npy\")\n",
    "\n",
    "np.save(\"../data/nn_data/img_x_test_rna_img.npy\", x_test_rna_img)\n",
    "np.save(\"../data/nn_data/img_x_test_rna_k.npy\", x_test_rna_k)\n",
    "y_test_rna.to_pickle(\"../data/nn_data/img_y_test_rna.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 361.5,
   "position": {
    "height": "40px",
    "left": "1035px",
    "right": "20px",
    "top": "120px",
    "width": "313px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
