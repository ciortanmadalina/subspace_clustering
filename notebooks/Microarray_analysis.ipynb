{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "This notebook apply the proposed method (GMM clustering) to a set of microarray datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T15:29:55.016025Z",
     "start_time": "2020-08-18T15:29:53.060707Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/dist-packages/scanpy/api/__init__.py:7: FutureWarning: \n",
      "\n",
      "In a future version of Scanpy, `scanpy.api` will be removed.\n",
      "Simply use `import scanpy as sc` and `import scanpy.external as sce` instead.\n",
      "\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "#GPU configuration\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)  # set this TensorFlow session as the default\n",
    "\n",
    "\n",
    "import random\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.cluster import adjusted_rand_score, normalized_mutual_info_score\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scripts.data_generator as data_generator\n",
    "import scripts.feature_ranking as feature_ranking\n",
    "import scripts.features_2d as features_2d\n",
    "import scripts.ga as ga\n",
    "import scripts.preprocess as preprocess\n",
    "import scripts.ga_evaluation as ga_evaluation\n",
    "import scripts.bio_analysis as bio_analysis\n",
    "import tensorflow as tf\n",
    "from IPython import get_ipython\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "\n",
    "plt.ion()\n",
    "plt.show()\n",
    "\n",
    "random_state=1\n",
    "random.seed( random_state )\n",
    "np.random.seed(random_state)\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps\n",
    "- Clustering 1d to select best discriminant features\n",
    "\n",
    "- Clustering 2d to select redundant, close and outlier features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T15:29:57.957807Z",
     "start_time": "2020-08-18T15:29:57.910736Z"
    }
   },
   "outputs": [],
   "source": [
    "random_state=0\n",
    "random.seed( random_state )\n",
    "np.random.seed(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T15:54:28.256243Z",
     "start_time": "2020-08-18T15:32:01.319937Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results = None\n",
    "filenames = np.array(['alon', 'borovecki', 'chiaretti', 'christensen', 'golub', 'gordon',\n",
    "       'khan', 'sorlie', 'su', 'yeoh', 'west'])\n",
    "clustering = \"gmm\"\n",
    "path = '../data/microarray/'\n",
    "method = \"adapted_ratkowsky_lance\"\n",
    "imp_f = np.arange(20)\n",
    "for name in filenames:\n",
    "    t1 = time.time()\n",
    "    data = pd.read_pickle(f'{path}' + name + '.pkl')\n",
    "    truth = data[\"truth\"].values\n",
    "    data = data.drop(\"truth\", axis = 1).values\n",
    "\n",
    "\n",
    "    n_clusters = len(np.unique(truth))\n",
    "\n",
    "    z_file= f\"../data/microarray/Z_{name}_correlation.npy\"\n",
    "    print(f\"\\n##########  {name}, {data.shape}\")\n",
    "\n",
    "    # Clustering 1D\n",
    "    meta_features = feature_ranking.rank_features(data,\n",
    "                                              nb_bins=20,\n",
    "                                              rank_threshold=90,\n",
    "                                              z_file=z_file,\n",
    "                                              metric='correlation',\n",
    "                                              redundant_threshold=0.4)\n",
    "    t2 = time.time()\n",
    "    print(f\"TIME: 1d Features : {(t2-t1)/60} min\")\n",
    "\n",
    "#     model_file = f'../models/gmm_arl.h5' \n",
    "\n",
    "#     population, n = features_2d.run(data,\n",
    "#                                 n_clusters,\n",
    "#                                 meta_features,\n",
    "#                                 model_file=model_file,\n",
    "#                                 add_close_population=False)\n",
    "    t3 = time.time()\n",
    "    t4 = time.time()\n",
    "    print(f\"TIME: 2d scores: {(t4-t3)/60} min\")\n",
    "    round_size = 3\n",
    "    epochs = 10*round_size\n",
    "\n",
    "    sampling = {\n",
    "    \"ARCHIVE2D\": { \n",
    "        \"ga\": 0,\n",
    "        \"max\": 0 },\n",
    "    \"CLOSE\": { \n",
    "        \"ga\": 0.35,\n",
    "        \"max\": 0.35 },\n",
    "    \"IMP1D\": { \n",
    "        \"ga\": 0.35,\n",
    "        \"max\": 0.35 },\n",
    "    \"RANDOM\": { \n",
    "        \"ga\": 0.3,\n",
    "        \"max\": 0.3},\n",
    "    }\n",
    "#     sampling = {\n",
    "#         \"ARCHIVE2D\": { \n",
    "#             \"ga\": 0.25,\n",
    "#             \"max\": 0.25 },\n",
    "#         \"CLOSE\": { \n",
    "#             \"ga\": 0.4,\n",
    "#             \"max\": 0.4 },\n",
    "#         \"IMP1D\": { \n",
    "#             \"ga\": 0.25,\n",
    "#             \"max\": 0.25 },\n",
    "#         \"RANDOM\": { \n",
    "#             \"ga\": 0.1,\n",
    "#             \"max\": 0.1},\n",
    "#         }\n",
    "    params = ga.ga_parameters(\n",
    "        n_clusters,\n",
    "        data.shape[1],\n",
    "        truth,\n",
    "        meta_features,\n",
    "        method=method,\n",
    "        truth_methods=['ari'],\n",
    "        archive_2d=None,#population[:data.shape[1] // 4],\n",
    "        epochs=epochs,\n",
    "        sampling = sampling,\n",
    "        round_size=round_size,\n",
    "        allow_subspace_overlap = True,\n",
    "        improvement_per_mutation_report = False,\n",
    "        clustering = clustering\n",
    "        \n",
    "    )\n",
    "    solutions, archive= ga.run(data, params)\n",
    "#     display(params[\"report\"].groupby([\"op\", \"improvement\"]).count())\n",
    "    solutions[\"dataset_name\"] = name\n",
    "    \n",
    "    t5 = time.time()\n",
    "    n_total = t5-t1\n",
    "    print(f\"TIME: GA: {(t5-t4)/60} min\")\n",
    "    print(f\"TIME: Total: {(t5-t1)/60} min\")\n",
    "    solutions[\"total_time\"] = round((t5-t1)/60, 1)\n",
    "    solutions[\"t(feature_sel)\"] = round((t2-t1)/60, 1)\n",
    "    solutions[\"t(cnn)\"] = round((t3-t2)/60, 1)\n",
    "    solutions[\"t(clustering2d)\"] = round((t4-t3)/60, 1)\n",
    "    solutions[\"t(ga)\"] = round((t5-t4)/60, 1)\n",
    "    \n",
    "    solutions[\"n_ga\"] = archive.shape[0]\n",
    "    solutions[\"n_cnn\"] = n_total\n",
    "    solutions[\"input_size\"] = data.shape[1]\n",
    "    \n",
    "    \n",
    "    solutions[\"n_non_redundant\"] = meta_features[(meta_features[\"redundant\"] ==1) ].shape[0]\n",
    "    solutions[\"n_imp\"] = meta_features[(meta_features[\"relevance\"] !=0)].shape[0]\n",
    "    solutions[\"n_imp4\"] = meta_features[(meta_features[\"relevance\"] ==4)].shape[0]\n",
    "    solutions[\"n_imp3\"] = meta_features[(meta_features[\"relevance\"] ==3)].shape[0]\n",
    "    \n",
    "    \n",
    "    if results is None: \n",
    "        results = solutions\n",
    "    else:\n",
    "        results = pd.concat([results, solutions], ignore_index = True)\n",
    "    results.to_excel(f\"../reports/microarray_{clustering}_{method}.xlsx\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_excel(\"../reports/microarray_gmm_adapted_ratkowsky_lance.xlsx\")\n",
    "\n",
    "max_ari = results.groupby(\"dataset_name\").agg({\"ari\": max}).reset_index()\n",
    "\n",
    "pd.merge(results[[\"dataset_name\", \"ari\", \"silhouette\"]], \n",
    "         max_ari, on = [\"dataset_name\", \"ari\"]).groupby(\"dataset_name\").max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = None\n",
    "filenames = np.array([\n",
    "    'alon', 'borovecki', 'chiaretti', 'christensen', 'golub', 'gordon',\n",
    "       'khan', 'sorlie', 'su', 'yeoh', 'west'])\n",
    "clustering = \"hdbscan\"\n",
    "path = '../data/microarray/'\n",
    "method = \"adapted_ratkowsky_lance\"\n",
    "imp_f = np.arange(20)\n",
    "for name in filenames:\n",
    "    t1 = time.time()\n",
    "    data = pd.read_pickle(f'{path}' + name + '.pkl')\n",
    "    truth = data[\"truth\"].values\n",
    "    data = data.drop(\"truth\", axis = 1).values\n",
    "\n",
    "\n",
    "    n_clusters = len(np.unique(truth))\n",
    "\n",
    "    z_file= f\"../data/microarray/Z_{name}_correlation.npy\"\n",
    "    print(f\"\\n##########  {name}, {data.shape}\")\n",
    "\n",
    "    # Clustering 1D\n",
    "    meta_features = feature_ranking.rank_features(data,\n",
    "                                              nb_bins=20,\n",
    "                                              rank_threshold=90,\n",
    "                                              z_file=z_file,\n",
    "                                              metric='correlation',\n",
    "                                              redundant_threshold=0.4)\n",
    "    t2 = time.time()\n",
    "    print(f\"TIME: 1d Features : {(t2-t1)/60} min\")\n",
    "\n",
    "    t3 = time.time()\n",
    "    t4 = time.time()\n",
    "    print(f\"TIME: 2d scores: {(t4-t3)/60} min\")\n",
    "    round_size = 3\n",
    "    epochs = 10*round_size\n",
    "\n",
    "    sampling = {\n",
    "    \"ARCHIVE2D\": { \n",
    "        \"ga\": 0,\n",
    "        \"max\": 0 },\n",
    "    \"CLOSE\": { \n",
    "        \"ga\": 0.35,\n",
    "        \"max\": 0.35 },\n",
    "    \"IMP1D\": { \n",
    "        \"ga\": 0.35,\n",
    "        \"max\": 0.35 },\n",
    "    \"RANDOM\": { \n",
    "        \"ga\": 0.3,\n",
    "        \"max\": 0.3},\n",
    "    }\n",
    "\n",
    "    params = ga.ga_parameters(\n",
    "        n_clusters,\n",
    "        data.shape[1],\n",
    "        truth,\n",
    "        meta_features,\n",
    "        method=method,\n",
    "        truth_methods=['ari'],\n",
    "        archive_2d=None,#population[:data.shape[1] // 4],\n",
    "        epochs=epochs,\n",
    "        sampling = sampling,\n",
    "        round_size=round_size,\n",
    "        allow_subspace_overlap = True,\n",
    "        improvement_per_mutation_report = False,\n",
    "        clustering = clustering\n",
    "        \n",
    "    )\n",
    "    solutions, archive= ga.run(data, params)\n",
    "#     display(params[\"report\"].groupby([\"op\", \"improvement\"]).count())\n",
    "    solutions[\"dataset_name\"] = name\n",
    "    \n",
    "    t5 = time.time()\n",
    "    n_total = t5-t1\n",
    "    print(f\"TIME: GA: {(t5-t4)/60} min\")\n",
    "    print(f\"TIME: Total: {(t5-t1)/60} min\")\n",
    "    solutions[\"total_time\"] = round((t5-t1)/60, 1)\n",
    "    solutions[\"t(feature_sel)\"] = round((t2-t1)/60, 1)\n",
    "    solutions[\"t(cnn)\"] = round((t3-t2)/60, 1)\n",
    "    solutions[\"t(clustering2d)\"] = round((t4-t3)/60, 1)\n",
    "    solutions[\"t(ga)\"] = round((t5-t4)/60, 1)\n",
    "    \n",
    "    solutions[\"n_ga\"] = archive.shape[0]\n",
    "    solutions[\"n_cnn\"] = n_total\n",
    "    solutions[\"input_size\"] = data.shape[1]\n",
    "    \n",
    "    \n",
    "    solutions[\"n_non_redundant\"] = meta_features[(meta_features[\"redundant\"] ==1) ].shape[0]\n",
    "    solutions[\"n_imp\"] = meta_features[(meta_features[\"relevance\"] !=0)].shape[0]\n",
    "    solutions[\"n_imp4\"] = meta_features[(meta_features[\"relevance\"] ==4)].shape[0]\n",
    "    solutions[\"n_imp3\"] = meta_features[(meta_features[\"relevance\"] ==3)].shape[0]\n",
    "    \n",
    "    \n",
    "    if results is None: \n",
    "        results = solutions\n",
    "    else:\n",
    "        results = pd.concat([results, solutions], ignore_index = True)\n",
    "    results.to_excel(f\"../reports/microarray_{clustering}_{method}.xlsx\")\n",
    "results.groupby(\"dataset_name\").agg({\"ari\": max})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_excel(\"../reports/microarray_hdbscan_adapted_ratkowsky_lance.xlsx\")\n",
    "\n",
    "max_ari = results.groupby(\"dataset_name\").agg({\"ari\": max}).reset_index()\n",
    "\n",
    "pd.merge(results[[\"dataset_name\", \"ari\", \"silhouette\"]], \n",
    "         max_ari, on = [\"dataset_name\", \"ari\"]).groupby(\"dataset_name\").max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = None\n",
    "filenames = np.array(['alon', 'borovecki', 'chiaretti', 'christensen', 'golub', 'gordon',\n",
    "       'khan', 'sorlie', 'su', 'yeoh'])\n",
    "clustering = \"leiden\"\n",
    "path = '../data/microarray/'\n",
    "method = \"adapted_ratkowsky_lance\"\n",
    "imp_f = np.arange(20)\n",
    "for name in filenames:\n",
    "    t1 = time.time()\n",
    "    data = pd.read_pickle(f'{path}' + name + '.pkl')\n",
    "    truth = data[\"truth\"].values\n",
    "    data = data.drop(\"truth\", axis = 1).values\n",
    "\n",
    "\n",
    "    n_clusters = len(np.unique(truth))\n",
    "\n",
    "    z_file= f\"../data/microarray/Z_{name}_correlation.npy\"\n",
    "    print(f\"\\n##########  {name}, {data.shape}\")\n",
    "\n",
    "    # Clustering 1D\n",
    "    meta_features = feature_ranking.rank_features(data,\n",
    "                                              nb_bins=20,\n",
    "                                              rank_threshold=90,\n",
    "                                              z_file=z_file,\n",
    "                                              metric='correlation',\n",
    "                                              redundant_threshold=0.4)\n",
    "    t2 = time.time()\n",
    "    print(f\"TIME: 1d Features : {(t2-t1)/60} min\")\n",
    "\n",
    "    t3 = time.time()\n",
    "    t4 = time.time()\n",
    "    print(f\"TIME: 2d scores: {(t4-t3)/60} min\")\n",
    "    round_size = 3\n",
    "    epochs = 10*round_size\n",
    "\n",
    "    sampling = {\n",
    "    \"ARCHIVE2D\": { \n",
    "        \"ga\": 0,\n",
    "        \"max\": 0 },\n",
    "    \"CLOSE\": { \n",
    "        \"ga\": 0.35,\n",
    "        \"max\": 0.35 },\n",
    "    \"IMP1D\": { \n",
    "        \"ga\": 0.35,\n",
    "        \"max\": 0.35 },\n",
    "    \"RANDOM\": { \n",
    "        \"ga\": 0.3,\n",
    "        \"max\": 0.3},\n",
    "    }\n",
    "#     sampling = {\n",
    "#         \"ARCHIVE2D\": { \n",
    "#             \"ga\": 0.25,\n",
    "#             \"max\": 0.25 },\n",
    "#         \"CLOSE\": { \n",
    "#             \"ga\": 0.4,\n",
    "#             \"max\": 0.4 },\n",
    "#         \"IMP1D\": { \n",
    "#             \"ga\": 0.25,\n",
    "#             \"max\": 0.25 },\n",
    "#         \"RANDOM\": { \n",
    "#             \"ga\": 0.1,\n",
    "#             \"max\": 0.1},\n",
    "#         }\n",
    "    params = ga.ga_parameters(\n",
    "        n_clusters,\n",
    "        data.shape[1],\n",
    "        truth,\n",
    "        meta_features,\n",
    "        method=method,\n",
    "        truth_methods=['ari'],\n",
    "        archive_2d=None,#population[:data.shape[1] // 4],\n",
    "        epochs=epochs,\n",
    "        sampling = sampling,\n",
    "        round_size=round_size,\n",
    "        allow_subspace_overlap = True,\n",
    "        improvement_per_mutation_report = False,\n",
    "        clustering = clustering\n",
    "        \n",
    "    )\n",
    "    solutions, archive= ga.run(data, params)\n",
    "#     display(params[\"report\"].groupby([\"op\", \"improvement\"]).count())\n",
    "    solutions[\"dataset_name\"] = name\n",
    "    \n",
    "    t5 = time.time()\n",
    "    n_total = t5-t1\n",
    "    print(f\"TIME: GA: {(t5-t4)/60} min\")\n",
    "    print(f\"TIME: Total: {(t5-t1)/60} min\")\n",
    "    solutions[\"total_time\"] = round((t5-t1)/60, 1)\n",
    "    solutions[\"t(feature_sel)\"] = round((t2-t1)/60, 1)\n",
    "    solutions[\"t(cnn)\"] = round((t3-t2)/60, 1)\n",
    "    solutions[\"t(clustering2d)\"] = round((t4-t3)/60, 1)\n",
    "    solutions[\"t(ga)\"] = round((t5-t4)/60, 1)\n",
    "    \n",
    "    solutions[\"n_ga\"] = archive.shape[0]\n",
    "    solutions[\"n_cnn\"] = n_total\n",
    "    solutions[\"input_size\"] = data.shape[1]\n",
    "    \n",
    "    \n",
    "    solutions[\"n_non_redundant\"] = meta_features[(meta_features[\"redundant\"] ==1) ].shape[0]\n",
    "    solutions[\"n_imp\"] = meta_features[(meta_features[\"relevance\"] !=0)].shape[0]\n",
    "    solutions[\"n_imp4\"] = meta_features[(meta_features[\"relevance\"] ==4)].shape[0]\n",
    "    solutions[\"n_imp3\"] = meta_features[(meta_features[\"relevance\"] ==3)].shape[0]\n",
    "    \n",
    "    \n",
    "    if results is None: \n",
    "        results = solutions\n",
    "    else:\n",
    "        results = pd.concat([results, solutions], ignore_index = True)\n",
    "    results.to_excel(\"../reports/microarray_leiden.xlsx\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.groupby(\"dataset_name\").agg({\"ari\": max})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import mixture\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import KMeans\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silhouette analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame()\n",
    "filenames = np.array([\n",
    "    'alon', 'borovecki', 'chiaretti', 'christensen', 'golub', 'gordon',\n",
    "       'khan', 'sorlie', 'su', 'yeoh', 'west'\n",
    "])\n",
    "clustering = \"hdbscan\"\n",
    "path = '../data/microarray/'\n",
    "\n",
    "for name in filenames:\n",
    "    print(name)\n",
    "    t1 = time.time()\n",
    "    data = pd.read_pickle(f'{path}' + name + '.pkl')\n",
    "    truth = data[\"truth\"].values\n",
    "    data = data.drop(\"truth\", axis = 1).values\n",
    "    if data.shape[1] > 8000:\n",
    "        data = data[:, :8000]\n",
    "    n_clusters = len(np.unique(truth))\n",
    "    row = {\"dataset\": name}\n",
    "    clustering = AffinityPropagation(random_state=5).fit(data)\n",
    "    ari = silhouette_score(data, clustering.labels_)\n",
    "    print(f\"Affinity {ari}\")\n",
    "    row[\"AffinityPropagation\"] = ari\n",
    "\n",
    "    clustering = SpectralClustering(n_clusters=n_clusters, assign_labels='discretize',random_state=5).fit(data)\n",
    "    ari = silhouette_score(data, clustering.labels_)\n",
    "    print(f\"Spectral {ari}\")\n",
    "    row[\"Spectral\"] = ari\n",
    "\n",
    "    clustering = KMeans(n_clusters=n_clusters,random_state=5).fit(data)\n",
    "    ari = silhouette_score(data, clustering.labels_)\n",
    "    print(f\"KMeans {ari}\")\n",
    "    row[\"KMeans\"] = ari\n",
    "\n",
    "    gmm = mixture.GaussianMixture(n_components=n_clusters,\n",
    "                  covariance_type=\"full\", random_state=0)\n",
    "    pred = gmm.fit_predict(data)\n",
    "    ari = silhouette_score(data, pred)\n",
    "    print(f\"GMM {ari}\")\n",
    "    row[\"GMM\"] = ari\n",
    "\n",
    "    pred = hdbscan.HDBSCAN(min_cluster_size =2).fit(data).labels_\n",
    "    ari = silhouette_score(data, pred)\n",
    "    print(f\"HDBSCAN {ari}\")\n",
    "    row[\"HDBSCAN\"] = ari\n",
    "    n_comp = min(data.shape[1], data.shape[0]) -1\n",
    "    pca = PCA(min(50, n_comp))\n",
    "    pca_data = pca.fit_transform(data)\n",
    "\n",
    "    clustering = KMeans(n_clusters=n_clusters,random_state=5).fit(pca_data)\n",
    "    ari = silhouette_score(pca_data, clustering.labels_)\n",
    "    print(f\"PCA KMeans {ari}\")\n",
    "    row[\"PCA_KMeans\"] = ari\n",
    "\n",
    "    gmm = mixture.GaussianMixture(n_components=n_clusters,\n",
    "                  covariance_type=\"full\", random_state=0)\n",
    "    pred = gmm.fit_predict(pca_data)\n",
    "    ari = silhouette_score(pca_data, pred)\n",
    "    print(f\"PCA GMM {ari}\")\n",
    "    row[\"PCA_GMM\"] = ari\n",
    "\n",
    "    pred = hdbscan.HDBSCAN(min_cluster_size =2).fit(pca_data).labels_\n",
    "    ari = silhouette_score(pca_data, pred)\n",
    "    print(f\"PCAHDBSCAN {ari}\")\n",
    "    row[\"PCA_HDBSCAN\"] = ari\n",
    "\n",
    "    results = results.append(row, ignore_index = True)\n",
    "    results.to_pickle(\"../data/microarray_others_silhouette.pkl\")\n",
    "# results.groupby(\"dataset_name\").agg({\"ari\": max})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARI analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame()\n",
    "filenames = np.array([\n",
    "#     'alon', \n",
    "#     'borovecki', \n",
    "    'chiaretti', 'christensen', 'golub', 'gordon',\n",
    "       'khan', 'sorlie', 'su', 'yeoh','west', ])\n",
    "clustering = \"hdbscan\"\n",
    "path = '../data/microarray/'\n",
    "method = \"adapted_ratkowsky_lance\"\n",
    "\n",
    "for name in filenames:\n",
    "    print(name)\n",
    "    t1 = time.time()\n",
    "    data = pd.read_pickle(f'{path}' + name + '.pkl')\n",
    "    truth = data[\"truth\"].values\n",
    "    data = data.drop(\"truth\", axis = 1).values\n",
    "    if data.shape[1] > 8000:\n",
    "        data = data[:, :8000]\n",
    "    n_clusters = len(np.unique(truth))\n",
    "    row = {\"dataset\": name}\n",
    "#     clustering = AffinityPropagation(random_state=5).fit(data)\n",
    "#     ari = adjusted_rand_score(truth, clustering.labels_)\n",
    "#     print(f\"Affinity {ari}\")\n",
    "#     row[\"AffinityPropagation\"] = ari\n",
    "\n",
    "#     clustering = SpectralClustering(n_clusters=n_clusters, assign_labels='discretize',random_state=5).fit(data)\n",
    "#     ari = adjusted_rand_score(truth, clustering.labels_)\n",
    "#     print(f\"Spectral {ari}\")\n",
    "#     row[\"Spectral\"] = ari\n",
    "\n",
    "#     clustering = KMeans(n_clusters=n_clusters,random_state=5).fit(data)\n",
    "#     ari = adjusted_rand_score(truth, clustering.labels_)\n",
    "#     print(f\"KMeans {ari}\")\n",
    "#     row[\"KMeans\"] = ari\n",
    "\n",
    "#     gmm = mixture.GaussianMixture(n_components=n_clusters,\n",
    "#                   covariance_type=\"full\", random_state=0)\n",
    "#     pred = gmm.fit_predict(data)\n",
    "#     ari = adjusted_rand_score(truth, pred)\n",
    "#     print(f\"GMM {ari}\")\n",
    "#     row[\"GMM\"] = ari\n",
    "\n",
    "#     pred = hdbscan.HDBSCAN(min_cluster_size =2).fit(data).labels_\n",
    "#     ari = adjusted_rand_score(truth, pred)\n",
    "#     print(f\"HDBSCAN {ari}\")\n",
    "#     row[\"HDBSCAN\"] = ari\n",
    "\n",
    "    n_comp = min(data.shape[0], data.shape[1])-1\n",
    "    pca = PCA(min(n_comp, 50))\n",
    "    pca_data = pca.fit_transform(data)\n",
    "\n",
    "    clustering = KMeans(n_clusters=n_clusters,random_state=5).fit(pca_data)\n",
    "    ari = adjusted_rand_score(truth, clustering.labels_)\n",
    "    print(f\"PCA KMeans {ari}\")\n",
    "    row[\"PCA_KMeans\"] = ari\n",
    "\n",
    "    gmm = mixture.GaussianMixture(n_components=n_clusters,\n",
    "                  covariance_type=\"full\", random_state=0)\n",
    "    pred = gmm.fit_predict(pca_data)\n",
    "    ari = adjusted_rand_score(truth, pred)\n",
    "    print(f\"PCA GMM {ari}\")\n",
    "    row[\"PCA_GMM\"] = ari\n",
    "\n",
    "    pred = hdbscan.HDBSCAN(min_cluster_size =2).fit(pca_data).labels_\n",
    "    ari = adjusted_rand_score(truth, pred)\n",
    "    print(f\"PCAHDBSCAN {ari}\")\n",
    "    row[\"PCA_HDBSCAN\"] = ari\n",
    "\n",
    "    results = results.append(row, ignore_index = True)\n",
    "    results.to_pickle(\"../data/microarray_others.pkl\")\n",
    "    print(results.shape)\n",
    "# results.groupby(\"dataset_name\").agg({\"ari\": max})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised analysis of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T19:10:31.360365Z",
     "start_time": "2020-07-24T19:10:16.014840Z"
    }
   },
   "outputs": [],
   "source": [
    "import scripts.ga_evaluation as ga_evaluation\n",
    "filenames = np.array(['alon', 'borovecki', 'chiaretti', 'christensen', 'golub', 'gordon',\n",
    "       'khan', 'sorlie', 'su', 'yeoh'])\n",
    "path = '../data/microarray/'\n",
    "imp_f = np.arange(20)\n",
    "result_df = pd.DataFrame()\n",
    "for name in filenames:\n",
    "    t1 = time.time()\n",
    "    data = pd.read_pickle(f'{path}' + name + '.pkl')\n",
    "    truth = data[\"truth\"].values\n",
    "    data = data.drop(\"truth\", axis = 1).values\n",
    "\n",
    "    n_clusters = len(np.unique(truth))\n",
    "\n",
    "    result = {\"Dataset\" : name,\n",
    "             \"Original Dimensions\": f\"{data.shape[0]} x {data.shape[1]}\",\n",
    "             \"Cluster sizes\" : \", \".join(np.array(list(Counter(truth).values())).astype(str))}\n",
    "    \n",
    "    predK = KMeans(n_clusters= n_clusters, random_state = 2).fit(data).labels_\n",
    "    ari_all = adjusted_rand_score(truth, predK)\n",
    "    \n",
    "    predK = KMeans(n_clusters= n_clusters, random_state = 2).fit(data[:, :5]).labels_\n",
    "    ari_top10 = adjusted_rand_score(truth, predK)\n",
    "    \n",
    "    pca = PCA(2)\n",
    "    pca_data = pca.fit_transform(data)\n",
    "\n",
    "    predK = KMeans(n_clusters= n_clusters, random_state = 2).fit(pca_data).labels_\n",
    "    ari_pca = adjusted_rand_score(truth, predK)\n",
    "    \n",
    "    r1 = ga_evaluation.random_sampling(data, truth, n_clusters, algo = \"gmm\")\n",
    "    r2 = ga_evaluation.random_sampling(data, truth, n_clusters, algo = \"hdbscan\")\n",
    "    result[\"ARI all dataset\"] = round(ari_all,2)\n",
    "    result[\"ARI PCA dataset\"] = round(ari_pca,2)\n",
    "    result[\"ARI top 10 features\"] = round(ari_top10,2)\n",
    "    result[\"Random GMM\"] = round(r1,2)\n",
    "    result[\"Random HDBSCAN\"] = round(r2,2)\n",
    "    result_df = result_df.append(result, ignore_index = True)\n",
    "\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run time analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-28T19:23:57.857801Z",
     "start_time": "2020-06-28T19:23:57.701095Z"
    }
   },
   "outputs": [],
   "source": [
    "results = pd.read_excel(\"reports/r_cnn.xlsx\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-28T19:23:58.607495Z",
     "start_time": "2020-06-28T19:23:58.567558Z"
    }
   },
   "outputs": [],
   "source": [
    "results[\"label\"] = results[\"dataset_name\"] + \" (\" +results[\"input_size\"].astype(str) + \" features) \" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-28T19:24:13.068223Z",
     "start_time": "2020-06-28T19:24:12.906692Z"
    }
   },
   "outputs": [],
   "source": [
    "perf = results.groupby(\"input_size\").min()[['t(feature_sel)', 't(cnn)','t(ga)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-28T19:24:20.121999Z",
     "start_time": "2020-06-28T19:24:20.084519Z"
    }
   },
   "outputs": [],
   "source": [
    "perf = perf.rename(columns = {\n",
    "    't(feature_sel)' : '1D Feature ranking', \n",
    "    't(cnn)' : '2D Feature ranking with NN',\n",
    "    't(ga)': 'Optimization algorithm for top 10 subspaces'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-28T19:24:22.999424Z",
     "start_time": "2020-06-28T19:24:22.550484Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,3))\n",
    "ax = plt.gca()\n",
    "perf.plot(kind='bar', stacked=True, ax = ax)\n",
    "plt.ylabel(\"time (min)\")\n",
    "plt.xticks(rotation = 0)\n",
    "sns.despine()\n",
    "plt.title(\"Run times on West, Khan, Gordon and Boroveki datasets\")\n",
    "plt.xlabel(\"Number of dimensions in the input dataset\")\n",
    "plt.savefig(f\"images/run_times.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best scores using supervised feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T16:00:16.514775Z",
     "start_time": "2020-08-18T16:00:16.471872Z"
    }
   },
   "outputs": [],
   "source": [
    "import scripts.ga_evaluation as ga_evaluation\n",
    "from sklearn import mixture\n",
    "import hdbscan\n",
    "filenames = np.array(['alon', 'borovecki', 'chiaretti', 'christensen', 'golub', 'gordon',\n",
    "       'khan', 'sorlie', 'su', 'yeoh', 'west'])\n",
    "path = '../data/microarray/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T11:38:39.941568Z",
     "start_time": "2020-08-18T11:38:33.936702Z"
    }
   },
   "outputs": [],
   "source": [
    "for name in filenames:\n",
    "    t1 = time.time()\n",
    "    data = pd.read_pickle(f'{path}' + name + '.pkl')\n",
    "    truth = data[\"truth\"].values\n",
    "    data = data.drop(\"truth\", axis = 1).values\n",
    "    print(Counter(truth))\n",
    "    n_clusters = len(np.unique(truth))\n",
    "    gmm_scores = []\n",
    "    hdbscan_scores = []\n",
    "    for i in range(2, 50):\n",
    "        input_data = data[:, :i]\n",
    "        gmm = mixture.GaussianMixture(n_components=n_clusters,\n",
    "                          covariance_type=\"full\", random_state=0)\n",
    "        pred = gmm.fit_predict(input_data)\n",
    "        ari = adjusted_rand_score(truth, pred)\n",
    "        gmm_scores.append(ari)\n",
    "\n",
    "        pred = hdbscan.HDBSCAN(min_cluster_size =2).fit(input_data).labels_\n",
    "        ari = adjusted_rand_score(truth, pred)\n",
    "        hdbscan_scores.append(ari)\n",
    "\n",
    "        \n",
    "    print(f\"\\n\\n\\n{name} GMM ari = {max(gmm_scores)}, \")\n",
    "    print(f\"{name} HDBSCAN ari = {max(hdbscan_scores)}, \")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T16:25:07.089941Z",
     "start_time": "2020-08-18T16:25:07.018998Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2,  mutual_info_classif, SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T16:27:30.101710Z",
     "start_time": "2020-08-18T16:25:07.715327Z"
    }
   },
   "outputs": [],
   "source": [
    "for name in filenames:\n",
    "    t1 = time.time()\n",
    "    data = pd.read_pickle(f'{path}' + name + '.pkl')\n",
    "    truth = data[\"truth\"].values\n",
    "    data = data.drop(\"truth\", axis = 1).values\n",
    "    print(Counter(truth))\n",
    "    n_clusters = len(np.unique(truth))\n",
    "    gmm_scores = []\n",
    "    hdbscan_scores = []\n",
    "    sel = SelectKBest(mutual_info_classif, k=50).fit_transform(data, truth)\n",
    "    for i in range(2, 50):\n",
    "        input_data = sel[:, :i]\n",
    "        gmm = mixture.GaussianMixture(n_components=n_clusters,\n",
    "                          covariance_type=\"full\", random_state=0)\n",
    "        pred = gmm.fit_predict(input_data)\n",
    "        ari = adjusted_rand_score(truth, pred)\n",
    "        gmm_scores.append(ari)\n",
    "\n",
    "        pred = hdbscan.HDBSCAN(min_cluster_size =2).fit(input_data).labels_\n",
    "        ari = adjusted_rand_score(truth, pred)\n",
    "        hdbscan_scores.append(ari)\n",
    "\n",
    "        \n",
    "    print(f\"\\n\\n\\n{name} GMM ari = {max(gmm_scores)}, \")\n",
    "    print(f\"{name} HDBSCAN ari = {max(hdbscan_scores)}, \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T11:40:10.328552Z",
     "start_time": "2020-08-18T11:40:09.406015Z"
    }
   },
   "outputs": [],
   "source": [
    "import scripts.ga_evaluation as ga_evaluation\n",
    "filenames = np.array(['alon', 'borovecki', 'chiaretti', 'christensen', 'golub', 'gordon',\n",
    "       'khan', 'sorlie', 'su', 'yeoh', 'west'])\n",
    "path = '../data/microarray/'\n",
    "\n",
    "for name in filenames:\n",
    "    t1 = time.time()\n",
    "    data = pd.read_pickle(f'{path}' + name + '.pkl')\n",
    "    truth = data[\"truth\"].values\n",
    "    input_data = data.drop(\"truth\", axis = 1).values\n",
    "    if len(input_data) > 5000:\n",
    "        input_data = input_data[:, :5000]\n",
    "\n",
    "    n_clusters = len(np.unique(truth))\n",
    "    gmm = mixture.GaussianMixture(n_components=n_clusters,\n",
    "                      covariance_type=\"full\", random_state=0)\n",
    "    pred = gmm.fit_predict(input_data)\n",
    "    ari = adjusted_rand_score(truth, pred)\n",
    "    print(f\"{name} GMM ari = {ari}\")\n",
    "\n",
    "    pred = hdbscan.HDBSCAN(min_cluster_size =2).fit(input_data).labels_\n",
    "    ari = adjusted_rand_score(truth, pred)\n",
    "    print(f\"{name} HDBSCAN ari = {ari}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-14T16:47:29.230420Z",
     "start_time": "2020-08-14T16:46:09.556264Z"
    }
   },
   "outputs": [],
   "source": [
    "import scripts.ga_evaluation as ga_evaluation\n",
    "filenames = np.array(['borovecki'])\n",
    "path = '../data/microarray/'\n",
    "\n",
    "for name in filenames:\n",
    "    t1 = time.time()\n",
    "    data = pd.read_pickle(f'{path}' + name + '.pkl')\n",
    "    truth = data[\"truth\"].values\n",
    "    input_data = data.drop(\"truth\", axis = 1).values\n",
    "    input_data = input_data[:, :10000]\n",
    "    \n",
    "\n",
    "    n_clusters = len(np.unique(truth))\n",
    "    \n",
    "    pred = hdbscan.HDBSCAN(min_cluster_size =2).fit(input_data).labels_\n",
    "    ari = adjusted_rand_score(truth, pred)\n",
    "    print(f\"{name} HDBSCAN ari = {ari}\")\n",
    "    \n",
    "    gmm = mixture.GaussianMixture(n_components=n_clusters,\n",
    "                      covariance_type=\"full\", random_state=0)\n",
    "    pred = gmm.fit_predict(input_data)\n",
    "    ari = adjusted_rand_score(truth, pred)\n",
    "    print(f\"{name} GMM ari = {ari}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution time analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_excel(\"reports/r_cnn.xlsx\", index_col=0)\n",
    "\n",
    "results[\"label\"] = results[\"dataset_name\"] + \" (\" +results[\"input_size\"].astype(str) + \" features) \" \n",
    "\n",
    "perf = results.groupby(\"input_size\").min()[['t(feature_sel)', 't(cnn)','t(ga)']]\n",
    "\n",
    "perf = perf.rename(columns = {\n",
    "    't(feature_sel)' : '1D Feature ranking', \n",
    "    't(cnn)' : '2D Feature ranking with NN',\n",
    "    't(ga)': 'Optimization algorithm for top 10 subspaces'\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize = (10,3))\n",
    "ax = plt.gca()\n",
    "perf.plot(kind='bar', stacked=True, ax = ax)\n",
    "plt.ylabel(\"time (min)\")\n",
    "plt.xticks(rotation = 0)\n",
    "sns.despine()\n",
    "plt.title(\"Run times on West, Khan, Gordon and Boroveki datasets\")\n",
    "plt.xlabel(\"Number of dimensions in the input dataset\")\n",
    "plt.savefig(f\"images/run_times.pdf\", bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 361.5,
   "position": {
    "height": "40px",
    "left": "1035px",
    "right": "20px",
    "top": "120px",
    "width": "313px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
